---
title: H2O & R
author: ''
date: '2024-01-25'
slug: h2o-r
categories: []
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2024-01-25T17:55:19-03:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```
## Introduction

This post provides a concise guide to applying H2O to the Bank Telemarketing dataset, familiar from our previous discussion. Our goal is to demonstrate how H2O's tools can be effectively used for predictive modeling. Remember to install Java for H2O to function correctly. Note that closing your R session will also end the H2O session.


### Libraries and Data Set Up

Load necessary libraries and the Bank Telemarketing dataset from the UCI repository. We remove the 'duration' feature to prevent look-ahead bias in our predictions and convert character variables to factors.

```{r}
library(tidyverse)         
library(rsample)
library(h2o)                
```

```{r}
# Load the data
data_url <- "http://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank.zip"
download.file(data_url, destfile = "bank.zip")
unzip("bank.zip", files = "bank.csv", exdir = "data")
bank_data <- read_csv2("data/bank.csv")
# Drop the duration feature as to avoid look ahead bias in our predictions
bank_data <- bank_data %>% mutate(across(where(is.character),as.factor))%>% 
    select(-duration)

```

### Data Preparation
Split the dataset into training and test sets, ensuring a stratified split based on the target variable 'y' for balanced representation.

```{r}
set.seed(123)
data_split <- initial_split(bank_data, prop = 0.75, strata = y)
train_data <- training(data_split)
test_data <- testing(data_split)
```

### H2O Initialization

Initialize an H2O instance, optimizing its performance by using all available cores and setting the maximum memory. Adjust the memory setting based on your dataset size and system capabilities. For best performance, the allocated memory should be 4x the size of your data, but never more than the total amount of memory on your computer.

```{r, message=FALSE, results='hide'}
# -1 uses all cores 
h2o.init(nthreads = -1, max_mem_size = "16g")
```
```{r, eval=FALSE}
# Check you cluster setting, make sure the choice of cores and memory size is correct
h2o.clusterInfo()
```


```{r, include=FALSE}
h2o.no_progress()
```

```{r}
# Clear out anything old
h2o.removeAll()
```

```{r, include = FALSE}
#First, let's read data into H2O. The `destination_frame` argument allows us to name the dataset in H2O.
#as.h2o() transfers data from R to the H2O instance.
#bank_data_h2o <- as.h2o(bank_data, destination_frame = "bank_data_h2o")
```

```{r, include=FALSE}
#bank_splits <- h2o.splitFrame(
#  data = bank_data_h2o,
#  ratios = c(0.75), 
#  seed = 123,
#  destination_frames = c("bank_train", "bank_test")
#)
#
#bank_train <- bank_splits[[1]]
#bank_test <- bank_splits[[2]]
```

Transfer the training and test datasets from R to the H2O environment. Use `h2o.ls()` to verify the its done.

```{r}
bank_train <- as.h2o(train_data)
bank_test <- as.h2o(test_data)
```

You should see two objects corresponding to the train and test sets.

```{r}
h2o.ls()
```
### Model Setup

Define the target and predictor variables for model building. Here we demonstrate training a Random Forest model with 1,000 trees, using 10-fold cross-validation and stratified sampling for robust model evaluation.

```{r}
y <- "y"
x <- setdiff(names(bank_data), y)
```
```{r, include=FALSE}
rf_model <- h2o.loadModel("D:/Users/user/Desktop/blog/content/post/2024-01-25-h2o-r/rf_h2o/rf_model")
```

```{r, eval=FALSE}
rf_model <- h2o.randomForest(y = y,
                           x = x,
                           training_frame = bank_train,
                           model_id = "rf_model",
                           nfolds = 10,
                           fold_assignment = "Stratified",
                           seed = 123,
                           max_runtime_secs = 240,
                           ntrees = 1000,
                           mtries = 4, #default value = sqrt(nÂºpredictors)
                           sample_rate = 0.632, #sample size per tree
                           min_rows = 1, #Minimum observations in a terminal node
                           max_depth = 20 #Maximum tree depth
                           )

```

View summaries of the Random Forest model, including key performance metrics like AUC (73.19%).

```{r, eval=FALSE}
summary(rf_model)
```
```{r, eval=FALSE}
h2o.performance(rf_model, 
                xval = TRUE)
h2o.confusionMatrix( rf_model, xval = TRUE)

h2o.gainsLift(rf_model, xval = TRUE)
```

Create a graph of variable importance.

```{r}
h2o.varimp_plot(rf_model)
```

Evaluate the model's performance on the test data to assess out-sample metrics.

```{r,include=FALSE, eval=FALSE}
test_perf <- h2o.performance(model = rf_model,
                newdata = bank_test)

h2o.confusionMatrix(test_perf)

h2o.gainsLift(rf_model)
```

```{r, include=FALSE}
# We can use the model to predict the outcome on new data.
#pred_test <- h2o.predict(rf_model,
#                         newdata = bank_test)
# make it an R data.frame
#as.data.frame(pred_test)
#We can also save the model. The `force = TRUE` will overwrite the model if it's already there.
#h2o.saveModel(rf_model,
#              path = "D:/Users/user/Desktop/Financial Data/rf_h2o", 
#              force = TRUE)
```

```{r, include=FALSE}
#And read it back in to use it later. 
#test <- h2o.loadModel(path = "")

# use the new model to predict on new data
#prediction_test <- h2o.predict(test,
#                         newdata = house_test)
# make it an R data.frame
#as.data.frame(prediction_test)
```

### Tuning the Model

In the initial model, default tuning parameters were used. Now, explore different combinations of 'mtries' and other parameters to optimize model performance.
```{r, include=FALSE}
best_rf <- h2o.loadModel("D:/Users/user/Desktop/blog/content/post/2024-01-25-h2o-r/rf_h2o/rf_tuning_model_1")
```

```{r, eval=FALSE}
rf_params <- list(mtries = c(3, 4, 7, 10), 
                  ntrees = 1000,
                  sample_rate = 0.632, #sample size per tree
                  min_rows = 1, 
                  max_depth = 20)

rf_mod_tune <- h2o.grid(
  "randomForest",
  y = y,
  x = x,
  training_frame = bank_train,
  grid_id = "rf_tuning",
  nfolds = 10, 
  fold_assignment = "Stratified",
  seed = 123,
  max_runtime_secs = 60,
  hyper_params = rf_params
  )
```

Next, we sort the results by AUC and examine the results from best model. Not sure why the default combination (mtry=4) is slightly different our previous rf_model.

```{r, eval=FALSE}
rf_perf <- h2o.getGrid(
  grid_id = "rf_tuning",
  sort_by = "auc"
)

best_model <- rf_perf@summary_table %>% 
    as.tibble() %>%
    mutate(id=row_number()) %>% 
    arrange(desc(auc)) %>% 
    slice(1)%>% 
    pull(id)

```

Use `h2o.getModel()` to choose the best model

```{r, eval=FALSE}
best_rf <- h2o.getModel(rf_perf@model_ids[[best_model]])
```

```{r, include=FALSE}
#h2o.saveModel(best_rf,
#              path = "D:/Users/user/Desktop/Financial Data/rf_h2o", 
#              force = TRUE)
```

Evaluate the best model on the test data.

```{r, eval=FALSE}
h2o.performance(model = best_rf,
                newdata = bank_test)
```

### Auto ML Function
Utilize H2O's AutoML for automated model selection and training, specifying constraints like the number of models and runtime.
```{r, include=FALSE}
best_auto <- h2o.loadModel("D:/Users/user/Desktop/blog/content/post/2024-01-25-h2o-r/rf_h2o/GBM_3_AutoML_1_20240125_150459")
```

```{r, eval=FALSE}
aml <- h2o.automl(
  y = y,
  x = x,
  training_frame = bank_train,
  nfolds = 10, 
  seed = 123,
  max_runtime_secs_per_model = 60,
  max_models = 10
)
```

Take a look at the leaderboard

```{r, eval=FALSE}
print(aml@leaderboard, 
      n = 10)
```

Extract the best model

```{r, eval=FALSE}
best_mod <- as.vector(aml@leaderboard$model_id)[1]
```

Save this model to use later. AUC 75.19%

```{r, eval=FALSE}
best_auto <- h2o.getModel(best_mod)
```
```{r, include=FALSE}
#h2o.saveModel(best_auto,
#              path = "D:/Users/user/Desktop/Financial Data/rf_h2o", 
#              force = TRUE)
```

Variable Importance plot.

```{r}
h2o.varimp_plot(best_auto)
```

Asses the performance on the test data. AUC 75.93%

```{r}
auto_pred <- h2o.performance(best_auto,
                         newdata = bank_test)

```

```{r, include=FALSE}
# Predict on new data
# auto_new_pred <- h2o.performance(best_auto,
#                         newdata = bank_test)
# auto_new_pred

```

### Closing the H2O Cluster

Finally, shut down the H2O cluster to free up resources.

```{r, eval=FALSE}
h2o.shutdown()
```